{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy hazm scikit-learn gensim transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# TODO: better dataset to use\n",
    "poems = list(line[1:-1] for line in \n",
    "             io.open('../datasets/Shahnameh-Poems.txt', mode=\"r\", encoding=\"utf-8\").readlines())[:50_000]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hazm\n",
    "\n",
    "# TODO: better stopwords to improve\n",
    "costums = [\n",
    "    'زین',\n",
    "    'مگر',\n",
    "    'گر',\n",
    "    'کز',\n",
    "    'پس',\n",
    "]\n",
    "stopwords = set(hazm.stopwords_list() + costums)\n",
    "\n",
    "poems = np.array(poems)\n",
    "poems = np.apply_along_axis(' '.join, 1, poems.reshape(-1, 2))\n",
    "\n",
    "df = pd.DataFrame(poems, columns=['poems'])\n",
    "\n",
    "normalizer = hazm.Normalizer(token_based=True)\n",
    "\n",
    "# find better persian poems normalization and cleaning\n",
    "def clean_poems(poem):\n",
    "    tokens = [tk for tk in hazm.word_tokenize(poem) if tk not in stopwords and len(tk) > 1]\n",
    "    text = ' '.join(tokens)\n",
    "    return normalizer.normalize(text)\n",
    "\n",
    "df['changed'] = df.poems.apply(clean_poems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poems</th>\n",
       "      <th>changed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22901</th>\n",
       "      <td>ز چندین سر و افسر نامدار چرا کرد رایت مرا خواستا</td>\n",
       "      <td>چندین سر افسر نامدار رایت مرا خواستا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8589</th>\n",
       "      <td>ز جهن و ز گرسیوز و هرک بود به کس راز نگشاد و شاد</td>\n",
       "      <td>جهن گرسیوز هرک کس راز نگشاد شاد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13564</th>\n",
       "      <td>بسازیم یکبار و جنگ‌آوریم بریشان در و کوه تنگ آور</td>\n",
       "      <td>بسازیم یکبار جنگ‌آوریم بریشان کوه تنگ آور</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>یکی نامه فرمایم اکنون به شاه فرستم به دست تو ای</td>\n",
       "      <td>نامه فرمایم شاه فرستم دست‌ای</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20521</th>\n",
       "      <td>جهان آفرین را دگر بود رای بهر کار با رای او نیست</td>\n",
       "      <td>جهان‌آفرین دگر رای بهر کار رای</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6798</th>\n",
       "      <td>همی بست بر باره رهام تنگ به برگستوان بر زده طوس</td>\n",
       "      <td>همی بست رهام تنگ برگستوان زده طوس</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15782</th>\n",
       "      <td>ده اسب گرانمایه زرین لگام نهاده برو داغ کاوس نام</td>\n",
       "      <td>ده اسب گرانمایه زرین لگام نهاده برو داغ کاوس نام</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22179</th>\n",
       "      <td>نماند کزین راستی بگذرم چو شاهان پیشین یپیچد سرم</td>\n",
       "      <td>نماند کزین راستی بگذرم چو شاهان پیشین یپیچد سرم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14572</th>\n",
       "      <td>چنین زندگانی نیارد بها که باشد سر اندر دم اژدها</td>\n",
       "      <td>زندگانی نیارد بها سر اندر دم اژدها</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20704</th>\n",
       "      <td>از اندیشه گردون مگر بگذرد ز رنج تو دیگر کسی برخو</td>\n",
       "      <td>اندیشه گردون مگر بگذرد رنج برخو</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  poems  \\\n",
       "22901  ز چندین سر و افسر نامدار چرا کرد رایت مرا خواستا   \n",
       "8589   ز جهن و ز گرسیوز و هرک بود به کس راز نگشاد و شاد   \n",
       "13564  بسازیم یکبار و جنگ‌آوریم بریشان در و کوه تنگ آور   \n",
       "2937   یکی نامه فرمایم اکنون به شاه فرستم به دست تو ای    \n",
       "20521  جهان آفرین را دگر بود رای بهر کار با رای او نیست   \n",
       "6798   همی بست بر باره رهام تنگ به برگستوان بر زده طوس    \n",
       "15782  ده اسب گرانمایه زرین لگام نهاده برو داغ کاوس نام   \n",
       "22179   نماند کزین راستی بگذرم چو شاهان پیشین یپیچد سرم   \n",
       "14572   چنین زندگانی نیارد بها که باشد سر اندر دم اژدها   \n",
       "20704  از اندیشه گردون مگر بگذرد ز رنج تو دیگر کسی برخو   \n",
       "\n",
       "                                                changed  \n",
       "22901              چندین سر افسر نامدار رایت مرا خواستا  \n",
       "8589                    جهن گرسیوز هرک کس راز نگشاد شاد  \n",
       "13564         بسازیم یکبار جنگ‌آوریم بریشان کوه تنگ آور  \n",
       "2937                       نامه فرمایم شاه فرستم دست‌ای  \n",
       "20521                    جهان‌آفرین دگر رای بهر کار رای  \n",
       "6798                  همی بست رهام تنگ برگستوان زده طوس  \n",
       "15782  ده اسب گرانمایه زرین لگام نهاده برو داغ کاوس نام  \n",
       "22179   نماند کزین راستی بگذرم چو شاهان پیشین یپیچد سرم  \n",
       "14572                زندگانی نیارد بها سر اندر دم اژدها  \n",
       "20704                   اندیشه گردون مگر بگذرد رنج برخو  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tfidf and Boolean approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# TODO: check bigram with ngram_range=(1, 2) and increase max_features\n",
    "pipe = Pipeline([('count', CountVectorizer(analyzer='word', ngram_range=(1, 1), max_features=15_000)),\n",
    "                 ('tfidf', TfidfTransformer(sublinear_tf=True))]).fit(df.changed)\n",
    "\n",
    "def get_document_vectors(series):\n",
    "    series = [clean_poems(doc) for doc in series]\n",
    "    boolw_vec = pipe['count'].transform(series).toarray().astype(bool).astype(int)\n",
    "    norm = np.linalg.norm(boolw_vec, axis=1).reshape(-1, 1)\n",
    "    return pipe.transform(series).toarray() , boolw_vec / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_documents, boolw_documents = get_document_vectors(df.poems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-10\n",
    "\n",
    "def get_similars_by_cosine_distance(vector, documents, n=5):\n",
    "    sq_vector = np.squeeze(vector)\n",
    "    similarity = documents.dot(sq_vector) / (np.linalg.norm(documents, axis=1) * np.linalg.norm(sq_vector) + eps)\n",
    "    sorted_indx = np.argsort(similarity)\n",
    "    \n",
    "    return list(zip(df.poems[sorted_indx[-n:]], similarity[sorted_indx[-n:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "tfidf: \"ز گیتی هنرمند و خامش توی که پروردگار سیاوش توی\" \t with similarity of 0.36\n",
      "tfidf: \"چپ لشکر و چنگ شیران توی نگهبان سالار ایران توی\" \t with similarity of 0.36\n",
      "tfidf: \"بدو گفت سالار و مهتر توی سیاووش رد را برادر توی\" \t with similarity of 0.37\n",
      "tfidf: \"ز شاهان گیتی گزیده توی جهانجوی و هم کار دیده توی\" \t with similarity of 0.38\n",
      "tfidf: \"که تاج سر شهریاران توی که گوید که پور شبانان توی\" \t with similarity of 1.00\n",
      "----------------------------------------------------------------------------------------------------\n",
      "boolw: \"کرانه گزید از بر تاج و گاه نهاده بر خود سر هر سه\" \t with similarity of 0.34\n",
      "boolw: \"به سر بر نهاد آن پدر داده تاج که زیبنده باشد بر \" \t with similarity of 0.34\n",
      "boolw: \"نه پور و برادر نه بوم و نه بر نه تاج و نه گنج و \" \t with similarity of 0.34\n",
      "boolw: \"به هنگام شیرین به دایه دهد یکی تاج زرینش بر سر ن\" \t with similarity of 0.34\n",
      "boolw: \"که تاج سر شهریاران توی که گوید که پور شبانان توی\" \t with similarity of 1.00\n"
     ]
    }
   ],
   "source": [
    "tfidf_vector, boolw_vector = \\\n",
    "    get_document_vectors(['که تاج سر شهریاران توی که گوید که پور شبانان توی'])\n",
    "\n",
    "# TODO: better format to report similarity\n",
    "print('-' * 100)\n",
    "for poem, sym in get_similars_by_cosine_distance(tfidf_vector, tfidf_documents):\n",
    "    print(\"tfidf: \\\"{}\\\" \\t with similarity of {:.2f}\".format(poem, sym))\n",
    "\n",
    "print('-' * 100)\n",
    "for poem, sym in get_similars_by_cosine_distance(boolw_vector, boolw_documents):\n",
    "    print(\"boolw: \\\"{}\\\" \\t with similarity of {:.2f}\".format(poem, sym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idfs = dict(zip(pipe['count'].get_feature_names_out(), pipe['tfidf'].idf_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining word embedding and idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from hazm import word_tokenize\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format('../models/farsi_literature_word2vec_model.txt')\n",
    "\n",
    "# TODO: check appropriate stopwords\n",
    "def embed(poem):\n",
    "    \n",
    "    poem = clean_poems(poem)\n",
    "    def get_wrod2vector(word):\n",
    "        return word2vec[word] if word in word2vec else np.zeros(100)\n",
    "    \n",
    "    embedding_vectors = [get_wrod2vector(wo) * word_idfs.get(wo, 0) for wo in word_tokenize(poem)]\n",
    "    return np.sum(embedding_vectors, axis=0).tolist()\n",
    "\n",
    "poems_embeddings = np.array(df.changed.apply(embed).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "embedding: \"که ای برتر از دانش پارسا جهاندار و بر هر کسی پاد\" \t with similarity of 0.68\n",
      "embedding: \"نیابد بدو نیز اندیشه راه که او برتر از نام و از \" \t with similarity of 0.68\n",
      "embedding: \"به یاران چنین گفت کاینت شگفت کزین برتر اندیشه نت\" \t with similarity of 0.69\n",
      "embedding: \"ستوده جهاندار برتر منش نخواهد که بر مابود سرزنش\" \t with similarity of 0.69\n",
      "embedding: \"به نام خداوند جان و خرد کزین برتر اندیشه برنگذرد\" \t with similarity of 1.00\n"
     ]
    }
   ],
   "source": [
    "sample_embedding = np.array(embed('به نام خداوند جان و خرد کزین برتر اندیشه برنگذرد'))\n",
    "\n",
    "# TODO: better format to report similarity\n",
    "print('-' * 100)\n",
    "for poem, sym in get_similars_by_cosine_distance(sample_embedding, poems_embeddings):\n",
    "    print(\"embedding: \\\"{}\\\" \\t with similarity of {:.2f}\".format(poem, sym))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use BigBird and ParsBert last hidden state as embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 15:22:11.089981: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/alireza/.mujoco/mujoco210/bin\n",
      "2022-06-05 15:22:11.090051: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Some weights of the model checkpoint at SajjadAyoubi/distil-bigbird-fa-zwnj were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdModel were not initialized from the model checkpoint at SajjadAyoubi/distil-bigbird-fa-zwnj and are newly initialized: ['bert.pooler.bias', 'bert.pooler.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BigBirdModel, AutoTokenizer, AutoConfig, AutoModel\n",
    "\n",
    "MODEL_NAME = \"SajjadAyoubi/distil-bigbird-fa-zwnj\"\n",
    "\n",
    "# TODO: check for fine-tuning\n",
    "model = BigBirdModel.from_pretrained(MODEL_NAME, attention_type=\"original_full\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find better n=? for faster embedding\n",
    "def batch_series(iterable, n=100):\n",
    "    length = len(iterable)\n",
    "    for ndx in range(0, length, n): yield iterable[ndx:min(ndx + n, length)]\n",
    "        \n",
    "def get_transformer_embedding(documents):\n",
    "    series = [clean_poems(doc) for doc in documents]\n",
    "    result = None\n",
    "    \n",
    "    for batch in batch_series(series):\n",
    "        output = model(**tokenizer(batch, return_tensors='pt', padding=True))\n",
    "        output = np.mean(output.last_hidden_state.detach().numpy(), axis=1)\n",
    "        \n",
    "        if result is None:\n",
    "            result = output\n",
    "        else:\n",
    "            result = np.concatenate((result, output))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems_embeddings = get_transformer_embedding(df.poems.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "embedding: \"غمی شد دل گیو و خیره بماند بدان خیرگی نام یزدان \" \t with similarity of 0.85\n",
      "embedding: \"وزان جایگه سوی دیو سپید بیامد به کردار تابنده شی\" \t with similarity of 0.85\n",
      "embedding: \"به یاران چنین گفت کاینت شگفت کزین برتر اندیشه نت\" \t with similarity of 0.85\n",
      "embedding: \"ز رستم چو بشنید خسرو سخن یکی دیگر اندیشه افگند ب\" \t with similarity of 0.85\n",
      "embedding: \"به نام خداوند جان و خرد کزین برتر اندیشه برنگذرد\" \t with similarity of 0.99\n"
     ]
    }
   ],
   "source": [
    "sample_embedding = get_transformer_embedding(['به نام خداوند جان و خرد کزین برتر اندیشه برنگذرد'])\n",
    "\n",
    "# TODO: better format to report similarity\n",
    "print('-' * 100)\n",
    "for poem, sym in get_similars_by_cosine_distance(sample_embedding, poems_embeddings):\n",
    "    print(\"embedding: \\\"{}\\\" \\t with similarity of {:.2f}\".format(poem, sym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems_embeddings = get_transformer_embedding(df.poems.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "embedding: \"چو بشنید گفتار سالار شاه برافراخت تا ماه فرخ کلا\" \t with similarity of 0.90\n",
      "embedding: \"ازیشان چو بشنید اسفندیار به جان و به تن دادشان ز\" \t with similarity of 0.90\n",
      "embedding: \"چو بشنید رستم پر اندیشه شد دلش از غم و درد چون ب\" \t with similarity of 0.90\n",
      "embedding: \"چو بشنید پاسخ‌گو پیلتن دلیران لشکر شدند انجمن\" \t with similarity of 0.90\n",
      "embedding: \"چو ضحاک بشنید اندیشه کرد ز خون پدر شد دلش پر ز د\" \t with similarity of 0.99\n"
     ]
    }
   ],
   "source": [
    "sample_embedding = get_transformer_embedding(['چو ضحاک بشنید اندیشه کرد ز خون پدر شد دلش پر ز د'])\n",
    "\n",
    "# TODO: better format to report similarity\n",
    "print('-' * 100)\n",
    "for poem, sym in get_similars_by_cosine_distance(sample_embedding, poems_embeddings):\n",
    "    print(\"embedding: \\\"{}\\\" \\t with similarity of {:.2f}\".format(poem, sym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "dp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
