{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "poems = list(line[1:-1] for line in \n",
    "             io.open('../datasets/Shahnameh-Poems.txt', mode=\"r\", encoding=\"utf-8\").readlines())[:1_000]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hazm\n",
    "\n",
    "poems = np.array(poems)\n",
    "poems = np.apply_along_axis(' '.join, 1, poems.reshape(-1, 2))\n",
    "\n",
    "df = pd.DataFrame(poems, columns=['poems'])\n",
    "\n",
    "normalizer = hazm.Normalizer(token_based=True)\n",
    "\n",
    "df['normal'] = df.poems.apply(normalizer.normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tfidf and Boolean approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "tfidf: \"بدو گفت من چاره سازم ترا به خورشید سر برفرازم تر\" \t with similarity of 0.20\n",
      "tfidf: \"جهاندار هوشنگ با رای و داد به جای نیا تاج بر سر \" \t with similarity of 0.20\n",
      "tfidf: \"برو تاختن کرد ناگاه مرگ نهادش به سر بر یکی تیره \" \t with similarity of 0.24\n",
      "tfidf: \"نشسته برو شهریاری چو ماه یکی تاج بر سر به جای کل\" \t with similarity of 0.24\n",
      "tfidf: \"به رشک اندر آهرمن بدسگال همی رای زد تا ببالید با\" \t with similarity of 0.33\n",
      "----------------------------------------------------------------------------------------------------\n",
      "boolw: \"برو تاختن کرد ناگاه مرگ نهادش به سر بر یکی تیره \" \t with similarity of 0.37\n",
      "boolw: \"به رشک اندر آهرمن بدسگال همی رای زد تا ببالید با\" \t with similarity of 0.37\n",
      "boolw: \"جهاندار هوشنگ با رای و داد به جای نیا تاج بر سر \" \t with similarity of 0.37\n",
      "boolw: \"سر بخت و تختش برآمد به کوه پلنگینه پوشید خود با \" \t with similarity of 0.39\n",
      "boolw: \"سر و تن بشستی نهفته به باغ پرستنده با او ببردی چ\" \t with similarity of 0.39\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('count', CountVectorizer(analyzer='word', ngram_range=(1, 1), max_features=20_000)),\n",
    "                 ('tfidf', TfidfTransformer(sublinear_tf=True))]).fit(df.normal)\n",
    "\n",
    "def get_document_vectors(series):\n",
    "    boolw_vec = pipe['count'].transform(series).toarray().astype(bool).astype(int)\n",
    "    norm = np.linalg.norm(boolw_vec, axis=1).reshape(-1, 1)\n",
    "    return pipe.transform(series).toarray() , boolw_vec / norm\n",
    "\n",
    "tfidf_documents, boolw_documents = get_document_vectors(df.normal)\n",
    "\n",
    "def get_similars_by_cosine_distance(vector, documents, n=5):\n",
    "    sq_vector = np.squeeze(vector)\n",
    "    similarity = documents.dot(sq_vector) / (np.linalg.norm(documents, axis=1) * np.linalg.norm(sq_vector))\n",
    "    sorted_indx = np.argsort(similarity)\n",
    "    \n",
    "    return list(zip(df.poems[sorted_indx[-n:]], similarity[sorted_indx[-n:]]))\n",
    "\n",
    "tfidf_vector, boolw_vector = \\\n",
    "    get_document_vectors(['رستم زد توی سر سیاوش و به گردآفرید گفت برو با بزرگترت بیا'])\n",
    "\n",
    "print('-' * 100)\n",
    "for poem, sym in get_similars_by_cosine_distance(tfidf_vector, tfidf_documents):\n",
    "    print(\"tfidf: \\\"{}\\\" \\t with similarity of {:.2f}\".format(poem, sym))\n",
    "\n",
    "print('-' * 100)\n",
    "for poem, sym in get_similars_by_cosine_distance(boolw_vector, boolw_documents):\n",
    "    print(\"boolw: \\\"{}\\\" \\t with similarity of {:.2f}\".format(poem, sym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tfidf_documents\n",
    "del boolw_documents\n",
    "\n",
    "word_idfs = dict(zip(pipe['count'].get_feature_names_out(), pipe['tfidf'].idf_))\n",
    "\n",
    "del pipe\n",
    "del tfidf_vector\n",
    "del boolw_vector\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining word embedding and idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "embedding: \"چو بنشست بر جایگاه مهی چنین گفت بر تخت شاهنشهی\" \t with similarity of 0.41\n",
      "embedding: \"برفت اهرمن را به افسون ببست چو بر تیزرو بارگی بر\" \t with similarity of 0.42\n",
      "embedding: \"برنجید و گسترد و خورد و سپرد برفت و به جز نام نی\" \t with similarity of 0.46\n",
      "embedding: \"به رشک اندر آهرمن بدسگال همی رای زد تا ببالید با\" \t with similarity of 0.48\n",
      "embedding: \"پژوهندهٔ نامهٔ باستان که از پهلوانان زند داستان\" \t with similarity of 0.48\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from hazm import word_tokenize\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format('../models/farsi_literature_word2vec_model.txt')\n",
    "\n",
    "def embed(poem):\n",
    "    embedding_vectors = [word2vec[wo] * word_idfs.get(wo, 0) for wo in word_tokenize(poem) if wo in word2vec]\n",
    "    return np.sum(embedding_vectors, axis=0).tolist()\n",
    "\n",
    "poems_embeddings = np.array(df.normal.apply(embed).tolist())\n",
    "\n",
    "sample_embedding = np.array(embed('رستم زد توی سر سیاوش و به گردافرید گفت برو با بزرگترت بیا'))\n",
    "\n",
    "print('-' * 100)\n",
    "for poem, sym in get_similars_by_cosine_distance(sample_embedding, poems_embeddings):\n",
    "    print(\"embedding: \\\"{}\\\" \\t with similarity of {:.2f}\".format(poem, sym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del word2vec\n",
    "del poems_embeddings\n",
    "del sample_embedding\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use BigBird and ParsBert last hidden state as embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 00:53:25.927552: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/alireza/.mujoco/mujoco210/bin\n",
      "2022-06-04 00:53:25.927574: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Some weights of the model checkpoint at SajjadAyoubi/distil-bigbird-fa-zwnj were not used when initializing BigBirdModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdModel were not initialized from the model checkpoint at SajjadAyoubi/distil-bigbird-fa-zwnj and are newly initialized: ['bert.pooler.bias', 'bert.pooler.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BigBirdModel, AutoTokenizer, AutoConfig, AutoModel\n",
    "\n",
    "MODEL_NAME = \"SajjadAyoubi/distil-bigbird-fa-zwnj\"\n",
    "\n",
    "model = BigBirdModel.from_pretrained(MODEL_NAME, attention_type=\"original_full\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer_embedding(documents):\n",
    "    output = model(**tokenizer(documents, return_tensors='pt', padding=True))\n",
    "    return np.mean(output.last_hidden_state.detach().numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "embedding: \"دل روشن من چو برگشت ازوی سوی تخت شاه جهان کرد رو\" \t with similarity of 0.83\n",
      "embedding: \"نپیچد کسی سر ز فرمان اوی نیارد گذشتن ز پیمان اوی\" \t with similarity of 0.83\n",
      "embedding: \"ز خارا گهر جست یک روزگار همی کرد ازو روشنی خواست\" \t with similarity of 0.85\n",
      "embedding: \"چو بیدار گشتم بجستم ز جای چه مایه شب تیره بودم ب\" \t with similarity of 0.85\n",
      "embedding: \"چو ضحاک بشنید اندیشه کرد ز خون پدر شد دلش پر ز د\" \t with similarity of 0.99\n"
     ]
    }
   ],
   "source": [
    "poems_embeddings = get_transformer_embedding(df.normal.tolist())\n",
    "\n",
    "sample_embedding = get_transformer_embedding(['چو ضحاک بشنید اندیشه کرد ز خون پدر شد دلش پر ز د'])\n",
    "                            \n",
    "print('-' * 100)\n",
    "for poem, sym in get_similars_by_cosine_distance(sample_embedding, poems_embeddings):\n",
    "    print(\"embedding: \\\"{}\\\" \\t with similarity of {:.2f}\".format(poem, sym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "embedding: \"سخن چون به گوش سیامک رسید ز کردار بدخواه دیو پلی\" \t with similarity of 0.87\n",
      "embedding: \"چو بشنید ازیشان سپهبد سخن یکی نامور نافه افکند ب\" \t with similarity of 0.88\n",
      "embedding: \"ز هرای درندگان چنگ دیو شده سست از خشم کیهان خدیو\" \t with similarity of 0.88\n",
      "embedding: \"چو آگه شد از مرگ فرزند شاه ز تیمار گیتی برو شد س\" \t with similarity of 0.89\n",
      "embedding: \"چو ضحاک بشنید اندیشه کرد ز خون پدر شد دلش پر ز د\" \t with similarity of 1.00\n"
     ]
    }
   ],
   "source": [
    "poems_embeddings = get_transformer_embedding(df.normal.tolist())\n",
    "\n",
    "sample_embedding = get_transformer_embedding(['چو ضحاک بشنید اندیشه کرد ز خون پدر شد دلش پر ز د'])\n",
    "                            \n",
    "print('-' * 100)\n",
    "for poem, sym in get_similars_by_cosine_distance(sample_embedding, poems_embeddings):\n",
    "    print(\"embedding: \\\"{}\\\" \\t with similarity of {:.2f}\".format(poem, sym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "dp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
